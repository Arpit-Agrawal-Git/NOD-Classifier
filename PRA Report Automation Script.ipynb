{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d0b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging Downloaded files\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder where your Excel files are stored\n",
    "folder_path = r\"C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Godrejdocs\\PRA\\24-25\\May\"  # Replace with your actual folder path\n",
    "\n",
    "# New Excel file to which the data will be appended\n",
    "output_file = r\"C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Godrejdocs\\PRA\\23-24\\Files\\All.xlsx\"\n",
    "\n",
    "# List to hold data from all files\n",
    "all_data = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.xls'):  # Adjusting for files that might be CSVs\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            # Attempt to read the file as CSV (tab-delimited), skipping bad lines\n",
    "            df = pd.read_csv(\n",
    "                file_path,\n",
    "                delimiter='\\t',\n",
    "                encoding='latin1',  # Using 'latin1' to avoid encoding issues\n",
    "                on_bad_lines='skip',  # Skipping bad lines\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read file {file_name}: {e}\")\n",
    "            continue\n",
    "        # Append the dataframe to the list\n",
    "        all_data.append(df)\n",
    "\n",
    "# Check if we have any dataframes to concatenate\n",
    "if not all_data:\n",
    "    print(\"No compatible files found. Please check the file formats.\")\n",
    "else:\n",
    "    # Concatenate all dataframes in the list\n",
    "    all_data_concatenated = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # Write the concatenated dataframe to a new Excel file\n",
    "    all_data_concatenated.to_excel(os.path.join(folder_path, output_file), index=False, engine='openpyxl')\n",
    "\n",
    "    print(f\"All data has been appended into {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5187b1fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been merged and saved into C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Godrejdocs\\PRA\\24-25\\May\\One.xlsx\n"
     ]
    }
   ],
   "source": [
    "#Adding Model and location 1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Input paths to the files\n",
    "main_file_path = r\"C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Godrejdocs\\PRA\\24-25\\May\\All.xlsx\"  # Replace with your main file path\n",
    "lookup_file_path = r\"C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Godrejdocs\\PRA\\23-24\\Sanctioned PRA 20-21.xlsx\"  # Replace with your lookup file path\n",
    "\n",
    "# New Excel file to which the merged data will be saved\n",
    "output_file_path = r\"C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Godrejdocs\\PRA\\24-25\\May\\One.xlsx\"  # Replace with your desired output file path\n",
    "\n",
    "# Read the main file and the lookup file into DataFrames\n",
    "main_df = pd.read_excel(main_file_path)\n",
    "lookup_df = pd.read_excel(lookup_file_path)\n",
    "\n",
    "# Drop duplicate 'Model Code' entries, keeping the first occurrence\n",
    "unique_lookup_df = lookup_df.drop_duplicates(subset='Model Code', keep='first')\n",
    "# Now create a dictionary for each 'Model Code' and its corresponding 'MODEL', 'FF/DC', 'LOCATION'\n",
    "model_code_mapping = unique_lookup_df.set_index('Model Code')[['MODEL', 'FF/DC', 'LOCATION']].to_dict('index')\n",
    "\n",
    "# Now, you can use the mapping to create the new columns in main_df\n",
    "main_df['MODEL'] = main_df['Model Code'].map(lambda x: model_code_mapping.get(x, {}).get('MODEL', None))\n",
    "main_df['FF/DC'] = main_df['Model Code'].map(lambda x: model_code_mapping.get(x, {}).get('FF/DC', None))\n",
    "main_df['LOCATION'] = main_df['Model Code'].map(lambda x: model_code_mapping.get(x, {}).get('LOCATION', None))\n",
    "\n",
    "# # Merge the additional columns based on 'Model Code'\n",
    "# # Assuming 'Model Code' is the column name in both DataFrames. Adjust if necessary.\n",
    "# merged_data = pd.merge(\n",
    "#     left=main_df,\n",
    "#     right=lookup_df[['Model Code', 'MODEL', 'FF/DC', 'LOCATION']],  # Adjust column names as necessary\n",
    "#     on='Model Code',  # This is the key for merging\n",
    "#     how='left'\n",
    "# )\n",
    "\n",
    "# Save the merged dataframe to a new Excel file\n",
    "main_df.to_excel(output_file_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data has been merged and saved into {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "538a40d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame is saved to C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Godrejdocs\\PRA\\24-25\\May\\Two.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Adding Model and Location 2 (New codes)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to the files\n",
    "main_file_path =  r\"C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Godrejdocs\\PRA\\24-25\\May\\One.xlsx\"\n",
    "second_lookup_file_path = r\"C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Godrejdocs\\ITEM CODES\\Supplier Location Upload Format (version 1).xlsx\" # Replace with the actual path\n",
    "\n",
    "# Read the files\n",
    "main_df = pd.read_excel(main_file_path)\n",
    "second_lookup_df = pd.read_excel(second_lookup_file_path,sheet_name='after_infor')\n",
    "\n",
    "# Drop duplicates in the second lookup DataFrame based on 'UNSPSC Code'\n",
    "second_lookup_df_unique = second_lookup_df.drop_duplicates(subset='UNSPSC Code', keep='last')\n",
    "\n",
    "# Create the mapping dictionaries for 'Product Group' and 'Supplier / Location' from the second lookup file\n",
    "model_mapping = second_lookup_df_unique.set_index('UNSPSC Code')['Product Group'].to_dict()\n",
    "location_mapping = second_lookup_df_unique.set_index('UNSPSC Code')['Supplier / Location'].to_dict()\n",
    "\n",
    "# Fill in missing 'MODEL' values\n",
    "main_df.loc[main_df['MODEL'].isna(), 'MODEL'] = main_df.loc[main_df['MODEL'].isna(), 'Defective Prod Code'].map(model_mapping)\n",
    "\n",
    "# Fill in missing 'LOCATION' values\n",
    "main_df.loc[main_df['LOCATION'].isna(), 'LOCATION'] = main_df.loc[main_df['LOCATION'].isna(), 'Defective Prod Code'].map(location_mapping)\n",
    "\n",
    "# Save the updated main_df to an Excel file\n",
    "output_file_path =r\"C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Godrejdocs\\PRA\\24-25\\May\\Two.xlsx\"\n",
    "main_df.to_excel(output_file_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Updated DataFrame is saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57dee509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame is saved to C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Godrejdocs\\PRA\\24-25\\May\\Three.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Adding Model and Location 3 (Baan Codes)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to the files\n",
    "main_file_path =  r\"C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Godrejdocs\\PRA\\24-25\\May\\Two.xlsx\"\n",
    "second_lookup_file_path = r\"C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Godrejdocs\\ITEM CODES\\Supplier Location Upload Format (version 1).xlsx\" # Replace with the actual path\n",
    "\n",
    "# Read the files\n",
    "main_df = pd.read_excel(main_file_path)\n",
    "second_lookup_df = pd.read_excel(second_lookup_file_path,sheet_name='main')\n",
    "\n",
    "# Drop duplicates in the second lookup DataFrame based on 'UNSPSC Code'\n",
    "second_lookup_df_unique = second_lookup_df.drop_duplicates(subset='Product Code', keep='last')\n",
    "\n",
    "# Create the mapping dictionaries for 'Product Group' and 'Supplier / Location' from the second lookup file\n",
    "model_mapping = second_lookup_df_unique.set_index('Product Code')['Product Group'].to_dict()\n",
    "location_mapping = second_lookup_df_unique.set_index('Product Code')['Supplier / Location'].to_dict()\n",
    "\n",
    "# Fill in missing 'MODEL' values\n",
    "main_df.loc[main_df['MODEL'].isna(), 'MODEL'] = main_df.loc[main_df['MODEL'].isna(), 'Defective Prod Code'].map(model_mapping)\n",
    "\n",
    "# Fill in missing 'LOCATION' values\n",
    "main_df.loc[main_df['LOCATION'].isna(), 'LOCATION'] = main_df.loc[main_df['LOCATION'].isna(), 'Defective Prod Code'].map(location_mapping)\n",
    "\n",
    "# Save the updated main_df to an Excel file\n",
    "output_file_path =r\"C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Godrejdocs\\PRA\\24-25\\May\\Three.xlsx\"\n",
    "main_df.to_excel(output_file_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Updated DataFrame is saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef95ab18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Read the files\n",
    "main_df = pd.read_excel(output_file_path)\n",
    "\n",
    "# Checking for empty/blanks in Model\n",
    "print(main_df[\"MODEL\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3b7c914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Godrejdocs\\PRA\\24-25\\May\\Four.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Adding other columns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Step 1: Read the file\n",
    "file_path = r\"C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Godrejdocs\\PRA\\24-25\\May\\Three.xlsx\"  # Replace with the path to your file\n",
    "main_df = pd.read_excel(file_path)\n",
    "\n",
    "# Step 2: Convert 'PRA Submitted Date' and 'Invoice Date' to datetime format\n",
    "main_df['PRA Submitted Date'] = pd.to_datetime(main_df['PRA Submitted Date'], format='mixed', dayfirst=True,errors='coerce')\n",
    "main_df['Invoice Date'] = pd.to_datetime(main_df['Invoice Date'], format='mixed',dayfirst=True,errors='coerce')\n",
    "\n",
    "# MONTH: Previous month in 'Mmm-YY' format\n",
    "last_month = datetime.today().replace(day=1) - timedelta(days=1)\n",
    "main_df['MONTH'] = last_month.strftime('%b-%y')\n",
    "\n",
    "# AGING\n",
    "main_df['AGING'] = (main_df['PRA Submitted Date'] - main_df['Invoice Date']).dt.days\n",
    "\n",
    "# AGING PERIOD & AGING CAT with updated conditions\n",
    "conditions = [\n",
    "    main_df['AGING'].between(0, 7),\n",
    "    main_df['AGING'].between(8, 15),\n",
    "    main_df['AGING'].between(16, 30),\n",
    "    main_df['AGING'].between(31, 60),\n",
    "    main_df['AGING'].between(61, 90),\n",
    "    main_df['AGING'].between(91, 180),\n",
    "    main_df['AGING'].between(181, 270),\n",
    "    main_df['AGING'].between(271, 365),\n",
    "    main_df['AGING'] > 365\n",
    "]\n",
    "period_choices = ['0-7 DAYS', '8-15 DAYS', '16-30 DAYS', '31-60 DAYS', '61-90 DAYS', '91-180 DAYS', '181-270 DAYS', '271-365 DAYS', 'ABOVE 365 DAYS']\n",
    "cat_choices = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I']\n",
    "\n",
    "main_df['AGING PERIOD'] = np.select(conditions, period_choices, default='Check Dates')\n",
    "main_df['AGING CAT'] = np.select(conditions, cat_choices, default='Check Dates')\n",
    "\n",
    "# YEAR: Assuming the execution year is the year needed\n",
    "fy_start = (datetime.now().year - 1) if datetime.now().month < 4 else datetime.now().year\n",
    "fy_end = fy_start + 1\n",
    "main_df['YEAR'] = f'{str(fy_start)[2:]}-{str(fy_end)[2:]}'\n",
    "\n",
    "# YR/M: Assuming the column exists with correct naming\n",
    "main_df['YR/M'] = main_df['Cabinet No(For Appl) and SO/RPL/Invoice No(For Others)'].astype(str).str[:4]\n",
    "\n",
    "# Save the DataFrame with changes to a new file\n",
    "new_file_path = r\"C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Godrejdocs\\PRA\\24-25\\May\\Four.xlsx\"  # Replace with the path to the new file\n",
    "main_df.to_excel(new_file_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"DataFrame saved to {new_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bb7e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a064d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c293f22bbcf414cb6b9163724e9b6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca98321189b945d1a7dde25ad12d3c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Job Codes:   0%|          | 0/867 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4621f41a3d644d94bf354df4db4a7f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing embeddings:   0%|          | 0/232 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa24b44a67d1415099ff954febc6d595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing embeddings:   0%|          | 0/17795 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158d64dec72340999d44350272c13aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing embeddings:   0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628d7737ee534d2e80f80bc287e42fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing embeddings:   0%|          | 0/2301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d31fb733edb49b5abee2fd4966e7eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing embeddings:   0%|          | 0/4539 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14052\\619373892.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mjob_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs_dict\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlookup_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Job Codes\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m     \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m     \u001b[0mlookup_embeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjob_code\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0memb\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14052\\619373892.py\u001b[0m in \u001b[0;36mcompute_embeddings\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mencoded_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[0mmodel_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_pooling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'attention_mask'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1511\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1518\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1522\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m         \u001b[0membedding_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1511\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1518\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1522\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    339\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m             layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    342\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1511\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1518\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1522\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# add self attentions if we output attention weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m         \u001b[0mintermediate_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1511\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1518\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1522\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1511\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1518\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1522\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Check if CUDA is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Function for mean pooling\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Load the tokenizer and model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(r\"C:\\Users\\arpitag\\all-mpnet-base-v2\")\n",
    "model = AutoModel.from_pretrained(r\"C:\\Users\\arpitag\\all-mpnet-base-v2\")\n",
    "\n",
    "# Load your dataframes\n",
    "main_df_path = r\"C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Godrejdocs\\PRA\\23-24\\May\\Four.xlsx\"\n",
    "lookup_df_path = r\"C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Godrejdocs\\PRA\\23-24\\Sanctioned PRA 20-21.xlsx\"\n",
    "output_df_path = r\"C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Godrejdocs\\PRA\\23-24\\May\\Five.xlsx\"\n",
    "\n",
    "lookup_df = pd.read_excel(lookup_df_path)\n",
    "main_df = pd.read_excel(main_df_path)\n",
    "\n",
    "# Fill NaN with empty strings to ensure all inputs are strings\n",
    "lookup_df.fillna('', inplace=True)\n",
    "main_df.fillna('', inplace=True)\n",
    "\n",
    "# Prepare the lookup dictionary with unique observations and their NOD\n",
    "lookup_dict = {}\n",
    "for _, row in tqdm(lookup_df.iterrows()):\n",
    "    job_code = row['JobCode 1']\n",
    "    observation = row['Observation/Defects']\n",
    "    nod = row['NOD']\n",
    "    if job_code not in lookup_dict:\n",
    "        lookup_dict[job_code] = {}\n",
    "    if observation not in lookup_dict[job_code]:\n",
    "        lookup_dict[job_code][observation] = nod\n",
    "\n",
    "# Function to safely compute embeddings, skipping errors\n",
    "def compute_embeddings(texts):\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts, desc=\"Computing embeddings\"):\n",
    "        try:\n",
    "            encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "            with torch.no_grad():\n",
    "                model_output = model(**encoded_input)\n",
    "            embeddings.append(mean_pooling(model_output, encoded_input['attention_mask']))\n",
    "        except Exception as e:\n",
    "            print(f\"Error with text: {text[:30]}... {e}\")\n",
    "            embeddings.append(torch.zeros(model.config.hidden_size))  # Append a zero vector for errors\n",
    "    return torch.stack(embeddings)\n",
    "\n",
    "# Compute embeddings for unique lookup observations\n",
    "lookup_embeddings = {}\n",
    "for job_code, obs_dict in tqdm(lookup_dict.items(), desc=\"Job Codes\"):\n",
    "    texts = list(obs_dict.keys())\n",
    "    embeddings = compute_embeddings(texts)\n",
    "    lookup_embeddings[job_code] = {text: emb for text, emb in zip(texts, embeddings)}\n",
    "\n",
    "# Process the main file\n",
    "results = []\n",
    "for _, row in tqdm(main_df.iterrows(), desc=\"Processing main dataset\"):\n",
    "    job_code = row['JobCode 1']\n",
    "    observation = row['Observation/Defects']\n",
    "    if job_code in lookup_embeddings:\n",
    "        observation_embedding = compute_embeddings([observation])[0]  # Compute embedding for the single observation\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = [torch.cosine_similarity(observation_embedding, lookup_emb.unsqueeze(0)).item() for lookup_emb in lookup_embeddings[job_code].values()]\n",
    "        max_sim_index = np.argmax(similarities)\n",
    "        max_sim_observation = list(lookup_dict[job_code].keys())[max_sim_index]\n",
    "        nod = lookup_dict[job_code][max_sim_observation]\n",
    "        results.append((nod, max(similarities)))\n",
    "    else:\n",
    "        results.append((None, 0))\n",
    "\n",
    "# Add results to the DataFrame and save\n",
    "main_df['NOD'], main_df['Similarity Score'] = zip(*results)\n",
    "main_df.to_excel(output_df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f57b8625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62d721dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Arrangement wrt master file\n",
    "\n",
    "\n",
    "# Load the two Excel files\n",
    "file1_df = pd.read_excel(r\"C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Godrejdocs\\PRA\\24-25\\May\\Five.xlsx\")\n",
    "file2_df = pd.read_excel(r\"C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Sanctioned PRA 20-21.xlsx\")\n",
    "\n",
    "# Get the column order from file2\n",
    "file2_columns = file2_df.columns.tolist()\n",
    "\n",
    "# Rearrange the columns in file1 to match the order in file2\n",
    "# Columns not found in file2 will be moved to the end in their original order\n",
    "file1_columns = file1_df.columns.tolist()\n",
    "new_order = [col for col in file2_columns if col in file1_columns] + [col for col in file1_columns if col not in file2_columns]\n",
    "\n",
    "rearranged_df = file1_df[new_order]\n",
    "\n",
    "# Save the rearranged DataFrame to a new Excel file\n",
    "rearranged_df.to_excel(r\"C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Godrejdocs\\PRA\\24-25\\May\\Final.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "669fccca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      "  0%|          | 0/38 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 1/38 [02:27<1:30:46, 147.21s/it]\u001b[A\n",
      "  5%|▌         | 2/38 [04:18<1:15:39, 126.11s/it]\u001b[A\n",
      "  8%|▊         | 3/38 [04:30<43:09, 74.00s/it]   \u001b[A\n",
      " 11%|█         | 4/38 [05:14<35:19, 62.34s/it]\u001b[A\n",
      " 13%|█▎        | 5/38 [05:22<23:24, 42.57s/it]\u001b[A\n",
      " 16%|█▌        | 6/38 [25:06<3:49:38, 430.57s/it]\u001b[A\n",
      " 18%|█▊        | 7/38 [25:35<2:34:44, 299.48s/it]\u001b[A\n",
      " 21%|██        | 8/38 [26:06<1:46:52, 213.75s/it]\u001b[A\n",
      " 24%|██▎       | 9/38 [26:50<1:17:40, 160.72s/it]\u001b[A\n",
      " 26%|██▋       | 10/38 [26:52<52:15, 111.97s/it] \u001b[A\n",
      " 29%|██▉       | 11/38 [27:00<35:57, 79.89s/it] \u001b[A\n",
      " 32%|███▏      | 12/38 [27:04<24:38, 56.85s/it]\u001b[A\n",
      " 34%|███▍      | 13/38 [27:49<22:15, 53.41s/it]\u001b[A\n",
      " 37%|███▋      | 14/38 [28:07<17:01, 42.57s/it]\u001b[A\n",
      " 39%|███▉      | 15/38 [28:39<15:10, 39.58s/it]\u001b[A\n",
      " 42%|████▏     | 16/38 [28:40<10:13, 27.88s/it]\u001b[A\n",
      " 45%|████▍     | 17/38 [28:46<07:24, 21.16s/it]\u001b[A\n",
      " 47%|████▋     | 18/38 [28:46<05:00, 15.01s/it]\u001b[A\n",
      " 50%|█████     | 19/38 [28:54<04:00, 12.66s/it]\u001b[A\n",
      " 55%|█████▌    | 21/38 [28:54<01:58,  6.99s/it]\u001b[A\n",
      " 58%|█████▊    | 22/38 [29:00<01:44,  6.56s/it]\u001b[A\n",
      " 61%|██████    | 23/38 [29:05<01:34,  6.33s/it]\u001b[A\n",
      " 63%|██████▎   | 24/38 [29:08<01:15,  5.40s/it]\u001b[A\n",
      " 66%|██████▌   | 25/38 [29:10<00:55,  4.29s/it]\u001b[A\n",
      " 68%|██████▊   | 26/38 [29:13<00:47,  3.93s/it]\u001b[A\n",
      " 71%|███████   | 27/38 [29:16<00:42,  3.85s/it]\u001b[A\n",
      " 74%|███████▎  | 28/38 [29:17<00:29,  2.93s/it]\u001b[A\n",
      " 76%|███████▋  | 29/38 [29:19<00:24,  2.67s/it]\u001b[A\n",
      " 79%|███████▉  | 30/38 [29:21<00:20,  2.57s/it]\u001b[A\n",
      " 82%|████████▏ | 31/38 [29:23<00:15,  2.27s/it]\u001b[A\n",
      " 84%|████████▍ | 32/38 [29:27<00:16,  2.71s/it]\u001b[A\n",
      " 87%|████████▋ | 33/38 [29:28<00:11,  2.29s/it]\u001b[A\n",
      " 89%|████████▉ | 34/38 [29:29<00:07,  1.81s/it]\u001b[A\n",
      " 92%|█████████▏| 35/38 [29:32<00:06,  2.18s/it]\u001b[A\n",
      " 95%|█████████▍| 36/38 [29:33<00:03,  1.91s/it]\u001b[A\n",
      " 97%|█████████▋| 37/38 [29:34<00:01,  1.54s/it]\u001b[A\n",
      "100%|██████████| 38/38 [29:34<00:00, 46.71s/it]\u001b[A\n",
      " 12%|█▎        | 1/8 [29:34<3:27:04, 1774.96s/it]\n",
      "  0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▎         | 1/28 [03:16<1:28:31, 196.71s/it]\u001b[A\n",
      "  7%|▋         | 2/28 [03:23<36:47, 84.90s/it]   \u001b[A\n",
      " 11%|█         | 3/28 [03:25<19:33, 46.93s/it]\u001b[A\n",
      " 14%|█▍        | 4/28 [03:28<11:52, 29.68s/it]\u001b[A\n",
      " 18%|█▊        | 5/28 [03:35<08:17, 21.63s/it]\u001b[A\n",
      " 21%|██▏       | 6/28 [03:38<05:37, 15.33s/it]\u001b[A\n",
      " 25%|██▌       | 7/28 [03:45<04:19, 12.37s/it]\u001b[A\n",
      " 29%|██▊       | 8/28 [03:45<02:50,  8.54s/it]\u001b[A\n",
      " 32%|███▏      | 9/28 [03:54<02:45,  8.71s/it]\u001b[A\n",
      " 36%|███▌      | 10/28 [03:56<02:02,  6.78s/it]\u001b[A\n",
      " 39%|███▉      | 11/28 [03:57<01:21,  4.81s/it]\u001b[A\n",
      " 43%|████▎     | 12/28 [03:57<00:56,  3.50s/it]\u001b[A\n",
      " 46%|████▋     | 13/28 [04:01<00:52,  3.53s/it]\u001b[A\n",
      " 50%|█████     | 14/28 [04:12<01:19,  5.67s/it]\u001b[A\n",
      " 54%|█████▎    | 15/28 [04:13<00:59,  4.55s/it]\u001b[A\n",
      " 57%|█████▋    | 16/28 [04:14<00:39,  3.29s/it]\u001b[A\n",
      " 61%|██████    | 17/28 [04:15<00:27,  2.53s/it]\u001b[A\n",
      " 64%|██████▍   | 18/28 [04:17<00:25,  2.57s/it]\u001b[A\n",
      " 68%|██████▊   | 19/28 [04:18<00:18,  2.10s/it]\u001b[A\n",
      " 71%|███████▏  | 20/28 [04:19<00:12,  1.62s/it]\u001b[A\n",
      " 75%|███████▌  | 21/28 [04:20<00:10,  1.57s/it]\u001b[A\n",
      " 79%|███████▊  | 22/28 [04:21<00:07,  1.23s/it]\u001b[A\n",
      " 82%|████████▏ | 23/28 [04:21<00:05,  1.10s/it]\u001b[A\n",
      " 86%|████████▌ | 24/28 [04:22<00:04,  1.09s/it]\u001b[A\n",
      " 89%|████████▉ | 25/28 [04:25<00:04,  1.36s/it]\u001b[A\n",
      " 93%|█████████▎| 26/28 [04:25<00:02,  1.10s/it]\u001b[A\n",
      " 96%|█████████▋| 27/28 [04:26<00:01,  1.11s/it]\u001b[A\n",
      "100%|██████████| 28/28 [04:27<00:00,  9.56s/it]\u001b[A\n",
      " 25%|██▌       | 2/8 [34:02<1:28:49, 888.26s/it] \n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|▊         | 1/13 [00:21<04:18, 21.55s/it]\u001b[A\n",
      " 15%|█▌        | 2/13 [01:35<09:39, 52.65s/it]\u001b[A\n",
      " 23%|██▎       | 3/13 [01:46<05:33, 33.34s/it]\u001b[A\n",
      " 31%|███       | 4/13 [02:42<06:19, 42.22s/it]\u001b[A\n",
      " 38%|███▊      | 5/13 [02:43<03:41, 27.63s/it]\u001b[A\n",
      " 46%|████▌     | 6/13 [02:46<02:13, 19.04s/it]\u001b[A\n",
      " 54%|█████▍    | 7/13 [02:47<01:19, 13.26s/it]\u001b[A\n",
      " 62%|██████▏   | 8/13 [02:48<00:45,  9.17s/it]\u001b[A\n",
      " 69%|██████▉   | 9/13 [02:49<00:26,  6.67s/it]\u001b[A\n",
      " 77%|███████▋  | 10/13 [02:50<00:15,  5.13s/it]\u001b[A\n",
      " 85%|████████▍ | 11/13 [02:51<00:07,  3.70s/it]\u001b[A\n",
      " 92%|█████████▏| 12/13 [02:51<00:02,  2.66s/it]\u001b[A\n",
      "100%|██████████| 13/13 [02:52<00:00, 13.24s/it]\u001b[A\n",
      " 38%|███▊      | 3/8 [36:54<46:46, 561.26s/it]  \n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|█▍        | 1/7 [00:01<00:06,  1.10s/it]\u001b[A\n",
      " 29%|██▊       | 2/7 [00:01<00:03,  1.53it/s]\u001b[A\n",
      " 43%|████▎     | 3/7 [00:01<00:02,  1.93it/s]\u001b[A\n",
      " 57%|█████▋    | 4/7 [00:02<00:01,  2.13it/s]\u001b[A\n",
      " 71%|███████▏  | 5/7 [00:02<00:00,  2.16it/s]\u001b[A\n",
      " 86%|████████▌ | 6/7 [00:03<00:00,  2.19it/s]\u001b[A\n",
      "100%|██████████| 7/7 [00:03<00:00,  2.08it/s]\u001b[A\n",
      " 50%|█████     | 4/8 [36:58<22:44, 341.02s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:03<00:06,  3.20s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:06<00:03,  3.09s/it]\u001b[A\n",
      "100%|██████████| 3/3 [00:12<00:00,  4.33s/it]\u001b[A\n",
      " 62%|██████▎   | 5/8 [37:11<11:08, 222.74s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\u001b[A\n",
      " 75%|███████▌  | 6/8 [37:11<04:54, 147.13s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:01,  2.72it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.97it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:01<00:00,  3.01it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:03<00:01,  1.20s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:04<00:00,  1.07it/s]\u001b[A\n",
      " 88%|████████▊ | 7/8 [37:16<01:40, 100.56s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 1/16 [00:00<00:07,  2.06it/s]\u001b[A\n",
      " 12%|█▎        | 2/16 [01:13<10:03, 43.12s/it]\u001b[A\n",
      " 19%|█▉        | 3/16 [01:17<05:28, 25.28s/it]\u001b[A\n",
      " 25%|██▌       | 4/16 [01:30<04:06, 20.50s/it]\u001b[A\n",
      " 31%|███▏      | 5/16 [01:45<03:23, 18.46s/it]\u001b[A\n",
      " 38%|███▊      | 6/16 [02:20<04:00, 24.03s/it]\u001b[A\n",
      " 44%|████▍     | 7/16 [02:24<02:37, 17.51s/it]\u001b[A\n",
      " 50%|█████     | 8/16 [02:27<01:42, 12.83s/it]\u001b[A\n",
      " 56%|█████▋    | 9/16 [02:28<01:03,  9.14s/it]\u001b[A\n",
      " 62%|██████▎   | 10/16 [02:28<00:38,  6.44s/it]\u001b[A\n",
      " 69%|██████▉   | 11/16 [02:29<00:22,  4.58s/it]\u001b[A\n",
      " 75%|███████▌  | 12/16 [02:30<00:15,  3.78s/it]\u001b[A\n",
      " 81%|████████▏ | 13/16 [02:31<00:08,  2.85s/it]\u001b[A\n",
      " 88%|████████▊ | 14/16 [02:33<00:05,  2.58s/it]\u001b[A\n",
      "100%|██████████| 16/16 [02:34<00:00,  9.63s/it]\u001b[A\n",
      "100%|██████████| 8/8 [39:50<00:00, 298.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# To create Photo n Video Directory for Monthly PRA Analysis\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define paths\n",
    "excel_path = r\"C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\Godrejdocs\\PRA\\23-24\\Mar\\MAR 24 ONE - Copy.xlsx\"\n",
    "folder_a = r\"C:\\PRA Image Project\\Processed\"  # Folder containing photos\n",
    "folder_b = r\"C:\\PRA Image Project\\PRA Videos\"  # Folder containing videos\n",
    "output_root = r\"C:\\PRA Analysis\\Mar 2024\"\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(excel_path)\n",
    "\n",
    "# Create the directory structure\n",
    "for category in tqdm(df['Product Category'].unique()):\n",
    "    # Ensure category is a string and not NaN\n",
    "    if pd.isna(category):\n",
    "        continue\n",
    "    category_path = os.path.join(output_root, str(category))\n",
    "    os.makedirs(category_path, exist_ok=True)\n",
    "    \n",
    "    for nod in tqdm(df[df['Product Category'] == category]['NOD'].unique()):\n",
    "        # Ensure nod is a string and not NaN\n",
    "        if pd.isna(nod):\n",
    "            continue\n",
    "        nod_path = os.path.join(category_path, str(nod))\n",
    "        os.makedirs(nod_path, exist_ok=True)\n",
    "        \n",
    "        for index, row in df[(df['Product Category'] == category) & (df['NOD'] == nod)].iterrows():\n",
    "            pra_no = row['PRA No']\n",
    "            yr_m_suffix = row['YR/M']\n",
    "            \n",
    "            # Ensure pra_no and yr_m_suffix are strings and not NaN\n",
    "            if pd.isna(pra_no) or pd.isna(yr_m_suffix):\n",
    "                continue\n",
    "            \n",
    "            for folder, file_types in [(folder_a, ['.jpg', '.jpeg', '.png']), (folder_b, ['.mp4', '.avi'])]:\n",
    "                for file in os.listdir(folder):\n",
    "                    if str(pra_no) in file and any(file.lower().endswith(ft) for ft in file_types):\n",
    "                        file_name, file_extension = os.path.splitext(file)\n",
    "                        mod_file_name = f\"{file_name}_{str(yr_m_suffix)}{file_extension}\"\n",
    "                        src = os.path.join(folder, file)\n",
    "                        dst = os.path.join(nod_path, mod_file_name)\n",
    "                        shutil.copy2(src, dst)\n",
    "\n",
    "print(\"Process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b8fc5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879f0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
